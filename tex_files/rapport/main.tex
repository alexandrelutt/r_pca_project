\documentclass[sigconf]{acmart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{bbm}

\renewcommand\footnotetextcopyrightpermission[1]{}
\newcommand{\sign}{\text{sign}}
\settopmatter{printacmref=false} % Removes citation information below abstract
\pagestyle{plain} % removes running headers

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{None}

\begin{document}

\title{Robust Principal Component Analysis on Graphs}

\author{Sofiane Ezzehi}
\affiliation{%
  \institution{École Normale Supérieure Paris-Saclay \\ École des Ponts ParisTech}
  \country{}
}
\email{sofiane.ezzehi@eleves.enpc.fr}

\author{Alexandre Lutt}
\affiliation{%
  \institution{École Normale Supérieure Paris-Saclay \\ École des Ponts ParisTech}
  \country{}
}
\email{alexandre.lutt@eleves.enpc.fr}

\begin{teaserfigure}
  \center
  \includegraphics[width=15cm, height=7cm]{sampleteaser}
  \caption{Low-rank reconstruction of corrupted images using the proposed method of~\cite{main_paper}.}
  \Description{Low-rank reconstruction of corrupted images.}
  \label{fig:teaser}
\end{teaserfigure}

\maketitle
\pagestyle{plain}

\section{Abstract}

Principal Component Analysis (PCA) is a very popular method for dimensionality reduction, and is used by thousands accross the world to provide 2D or 3D visualisations and insights about high-dimension data. It is used in a variety of different fields, such as image processing, finance, biology, and computer vision, to name only a few of them.
In this work, we present a benchmark of different variants of PCA, which aim to tackle some of the issues of the original PCA algorithm that prevent it from being used in many real-life situations. 

\section{Introduction}

As mentionned in the abstract, PCA is a useful tool for dimensionnality reduction. More specifically, given a matrix $X \in \mathbb{R}^{n \times p}$, it solves the following optimization problem:

\begin{equation*}
  \begin{aligned}
  & \underset{Q, U}{\min}
  & & ||X - UQ^T||_F^2 \\
  & \text{subject to}
  & & Q^TQ = I \\
  \end{aligned}
\end{equation*}

  where $||.||_F$ designates the Frobenius norm. It can be shown that this problem actually has a closed-form solution given by the principal components of $X$.

However, the main drawback of PCA is that it is very sensitive to noise, corrupted data and outliers and thus cannot be used in many real-world applications.
The sensitivity to noise has been partially solved by the introduction of a robust variant of PCA, RPCA~\cite{rpca_paper}. However, this algorithm is very slow on large datasets, which makes it impractical for many applications. 
This motivated the introduction of another PCA variant called GLPCA~\cite{glpca_paper} which uses graph Laplacian regularization to improve the robustness of the algorithm while keeping the computational cost low.
Finally, a third variant of PCA similar to the previous ones has been proposed by the authors of~\cite{main_paper} to improve the robustness of the algorithm while keeping the computational cost low.

This project aims to provide a simple and efficient implementation of those main variants of the PCA algorithm, as well as a benchmark of those methods on different tasks (clustering and low-rank recovery for corrupted data on real-life and artificial datsets).

\section{Algorithms}

In this section, we are going to present the algorithms we implemented and compared for the benchmark.
We can divide them into two categories: factorized and non-factorized models. The first category regroups the original PCA algorithm and its variants which aim to learn two matrix factors $U$ and $Q$ such that $X \approx UQ$. Those models often have the constraint $Q^TQ= I$, which, in the case of the original PCA, can be understood as the orthonormality condition of the eigenvectors basis of $X^TX$. On the other hand, the second category regroups PCA variants which try to learn a low-rank matrix $L$ such that $X = L + S$, where $S$ is sparse.
For example, the original PCA algorithm, as well as GLPCA and RGLPCA are factorized models, while RPCA and the new variant introduced in~\cite{main_paper} are non-factorized models.

\subsection{Classical PCA} 

We used Scikit-Learn implementation of PCA, which simply computes the SVD of $X$ and creates a low-rank approximation by using the first $k$ singular vectors of $X$.

\subsection{Robust PCA}

We implemented (see algorithm~\ref{alg:rpca}) the algorithm described in~\cite{rpca_paper} using the ADMM~\cite{admm_paper} (Alternating Direction Method of Multipliers) and following as closely as possible the pseudo-code provided in the paper.
  This algorithm solves the following convex optimization problem:

  \begin{equation*}
    \begin{aligned}
    & \underset{L, S}{\min}
    & & ||L||_* + \lambda ||S||_1 \\
    & \text{subject to}
    & & L + S = X \\
    \end{aligned}
  \end{equation*}

  where $||.||_*$ is the nuclear norm and $||.||_1$ is the $\ell_1$ norm. This problem is solved iteratively by alterning between phases of optimization over $L$ and over $S$.
  
\subsection{GLPCA}

We also implemented (see algorithm~\ref{alg:glpca}) the algorithm described in~\cite{glpca_paper} using the closed-form solution proved in the original paper.
  
  This algorithm solves the following optimization problem:

  \begin{equation*}
    \begin{aligned}
    & \underset{Q, U}{\min}
    & & ||X - UQ^T||_F^2 + \alpha Tr(Q^T (D-W) Q) \\
    & \text{subject to}
    & & Q^TQ = I \\
    \end{aligned}
  \end{equation*}

  Where $W$ is the adjacency matrix of the graph $\mathcal{G}$, $D$ is the degree matrix of $\mathcal{G}$, and $\alpha$ is a hyperparameter of the algorithm. In pratice, we followed the pseudo-code provided in paper~\cite{glpca_paper} and used the closed-form solution obtained when reformulating the problem using $\alpha = \dfrac{\beta}{1 - \beta} \dfrac{\lambda_n}{\xi_n}$ (where $\lambda_n$ and $\xi_n$ are respectively the maximum eigenvalues of $X^TX$ and $D-W$) and differentiating the objective function with respect to $Q$ and $U$.

\subsection{RGLPCA}

We also implemented (see algorithm~\ref{alg:rglpca}) the robust version of the previous algorithm also mentionned in~\cite{glpca_paper}. This algorithm solves the following optimization problem:
  
  \begin{equation*}
    \begin{aligned}
    & \underset{Q, U}{\min}
    & & ||X - UQ^T||_{2, 1} + \alpha Tr(Q^T (D-W) Q) \\
    & \text{subject to}
    & & Q^TQ = I \\
    \end{aligned}
  \end{equation*}

  where $||.||_{2, 1}$ is the $\ell_{2, 1}$ norm defined by $||A||_{2, 1} = \displaystyle \sum\limits_{j=1}^n \sqrt{\sum \limits_{i=1}^p A_{ij}^2}$. This problem is solved using the ADMM~\cite{admm_paper} algorithm, following the pseudo-code provided in the paper.
  
\subsection{RPCA on graphs}

Finally, we implemented (see algorithm~\ref{alg:our_pca}) the RPCA on graphs algorithm described in~\cite{main_paper} which claims to be more robust than the previous ones while keeping the computational cost low. This algorithm solves the following convex optimization problem:
  
  \begin{equation*}
    \begin{aligned}
    & \underset{L, S}{\min}
    & & ||L||_* + \lambda ||S||_1 + \gamma Tr(L \phi L^T) \\
    & \text{subject to}
    & & L + S = X\\
    \end{aligned}
  \end{equation*}

  Where $\phi$ is the graph Laplacian of $\mathcal{G}$ and $\gamma$ is a hyperparameter of the algorithm. As the previous methods, it uses the ADMM~\cite{admm_paper} algorithm to solve this problem by alterning phases of optimization over $L$ and over $S$.

\section{Pseudo-code}

In order to make notations more concise, we will define the following shrinkage and singular value tresholding operators (defined on the space of square matrices):

\begin{equation*}
  \begin{cases}
    \mathcal{S}_{\tau}(\mathbf{Z}) = \sign{(\mathbf{Z})} \times \max{(\mathbf{0}, |\mathbf{Z}| - \tau \mathbf{I})} \\
      \mathcal{D}_{\tau}(\mathbf{Z}) = \mathbf{U} \mathcal{S}_{\tau}(\mathbf{\Sigma}) \mathbf{V}^T \text{ with } \mathbf{Z} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
  \end{cases}
\end{equation*}

\begin{algorithm}
  \caption{RPCA algorithm}\label{alg:rpca}
  \begin{algorithmic}
      \Require $\mathbf{X} \in \mathbb{R}^{p \times n}$, $\varepsilon$, $n_{iter}$
      
      \State $\lambda \gets \frac{1}{\sqrt{\max(m, n)}}$
      \State $\mu \gets \frac{\max(m, n)}{4 \times ||\mathbf{X}||_1}$
      \State $\mathbf{S} \gets \mathbf{0}$
      \State $\mathbf{Y} \gets \mathbf{0}$
      
      \For{$i = 1$ \textbf{to} $n_{iter}$}
          \State $\mathbf{L} \gets \mathcal{D}_{\mu}(\mathbf{X} - \mathbf{S} + \frac{1}{\mu} \mathbf{Y})$
          \State $\mathbf{S} \gets \mathcal{S}_{\frac{\lambda}{\mu}} (\mathbf{X} - \mathbf{L} + \frac{1}{\mu} \mathbf{Y})$ 
          \State $\mathbf{Y} \gets \mathbf{Y} + \mu(\mathbf{X} - \mathbf{L} - \mathbf{S})$
          \If{$||\mathbf{X} - \mathbf{L} - \mathbf{S}||_{F} \leqslant \varepsilon$}
              \State break\
          \EndIf
      \EndFor
      
      \State \Return $\mathbf{L}$, $\mathbf{S}$
  \end{algorithmic}
  \end{algorithm}
  
  \begin{algorithm}
  \caption{GLPCA algorithm}\label{alg:glpca}
  \begin{algorithmic}
      \Require $\mathbf{X} \in \mathbb{R}^{p \times n}$, $\mathcal{G}$,  $\beta$
      
      \State $\mathbf{W} \gets adj(\mathcal{G})$
      \State $\mathbf{D} \gets diag(\{d_1, d_2, ..., d_{n_{\text{nodes}}}\})$
      \State $\mathbf{L} \gets \mathbf{D} - \mathbf{W}$
      
      \State $\lambda_n \gets \Re(\max(eigenval(\mathbf{X}^T\mathbf{X})))$
      \State $\xi_n \gets \Re(\max(eigenval(\mathbf{L})))$
      
      \State $\mathbf{G}_{\beta} \gets (1 - \beta)(\mathbf{I} - \frac{1} {\lambda_n}\mathbf{X}^T\mathbf{X}) + \frac{\beta}{\xi_n}\mathbf{L}$
      
      \State $\mathbf{Q} \gets \Re{(eigenvect(\mathbf{G}_{\beta}))}$
      \State $\mathbf{U} \gets \mathbf{XQ}$
      
      \State \Return $\mathbf{Q}$, $\mathbf{U}$
  \end{algorithmic}
  \end{algorithm}
  
  
  \begin{algorithm}[H]
  \caption{RGLPCA algorithm}\label{alg:rglpca}
  \begin{algorithmic}
      \Require $\mathbf{X} \in \mathbb{R}^{p \times n}$, $\mathcal{G}$, $\beta$, $k$, $\rho$, $n_{iter}$
      
      \State $\mathbf{X}_0 \gets \mathbf{X}$
      \State $\beta_0 \gets \beta$
      
      \State $\mathbf{E} \gets \bm{1}$
      \State $\mathbf{C} \gets \bm{1}$
      \State $\mu \gets 1$
      
      \State $\mathbf{W} \gets adj(\mathcal{G})$
      \State $\mathbf{D} \gets diag(\{d_1, d_2, ..., d_{n_{\text{nodes}}}\})$
      \State $\mathbf{L} \gets \mathbf{D} - \mathbf{W}$
      
      \State $\xi \gets \Re(max(eigenval(\mathbf{X}_0^T \mathbf{X}_0)))$
      
      \For{$i = 1$ \textbf{to} $n_{iter}$}
          \State $\mathbf{X}_i \gets \mathbf{X} - \mathbf{E} - \frac{1}{\mu}\mathbf{C}$
          \State $\lambda \gets \Re(\max(eigenval(\mathbf{X}_i^T \mathbf{X}_i)))$
      
          \State $\alpha \gets \frac{2 \beta_0 \lambda}{\mu (1 - \beta_0)\xi}$
          \State $\beta \gets \frac{\alpha \xi}{\lambda + \alpha \xi}$
      
          \State $\mathbf{Q}, \mathbf{U} \gets GLPCA(\mathbf{X}, \mathcal{G}, \beta, k)$
      
          \State $\mathbf{A} \gets \mathbf{X}_k - \mathbf{UQ}^T - \frac{1}{\mu} \mathbf{C}$
          \State $\mathbf{a} \gets (||\mathbf{A}_1||_2, ||\mathbf{A}_2||_2, ..., ||\mathbf{A}_m||_2)$
          \State $\mathbf{E} \gets \mathcal{S}_{\frac{1}{\mu}}(\mathbf{a})$
          \State $\mathbf{C} \gets \mathbf{C} + \mu(\mathbf{E} - \mathbf{X} - \mathbf{UQ}^T)$
          \State $\mu \gets \rho \mu$
          
      \EndFor
      
      \State \Return $\mathbf{Q}$, $\mathbf{U}$, $\mathbf{E}$
  \end{algorithmic}
  \end{algorithm}
  
  
  \begin{algorithm}[H]
  \caption{RPCA on graphs algorithm}\label{alg:our_pca}
  \begin{algorithmic}
      \Require $\mathbf{X} \in \mathbb{R}^{p \times n}$, $\mathcal{G}$, $n_{iter}$, $\gamma$, $\varepsilon$
      
      \State $\mathbf{A} \gets adj(\mathcal{G})$
      \State $\mathbf{D} \gets diag(\{d_1, d_2, ..., d_{n_{\text{nodes}}}\})$
      
      \State $\bm{\phi} \gets D^{-\frac{1}{2}} A D^{-\frac{1}{2}} $
      
      \State $\lambda \gets \frac{1}{\sqrt{\max(p, n)}}$
      \State $\mathbf{L} \gets random(p, n)$
      \State $\mathbf{W} \gets random(p, n)$
      \State $\mathbf{S} \gets random(p, n)$
      
      \State $r_1 \gets 1$
      \State $r_2 \gets 1$
      
      \State $\mathbf{Z}_1 \gets \mathbf{X} - \mathbf{L} - \mathbf{S}$
      \State $\mathbf{Z}_2 \gets \mathbf{W} - \mathbf{L}$
      
      \For{$i = 1$ \textbf{to} $n_{iter}$}
          \State $\mathbf{H}_1 \gets \mathbf{X} - \mathbf{S} + \frac{1}{r_1}\mathbf{Z}_1$
          \State $\mathbf{H}_2 \gets \mathbf{W} + \frac{1}{r_2}\mathbf{Z}_2$
          \State $\mathbf{H} \gets \frac{1}{r_1 + r_2} (r_1 \mathbf{H}_1 + r_2 \mathbf{H}_2)$
          \State $\mathbf{L} \gets \mathcal{D}_{\frac{2}{r_1 + r_2}}(\mathbf{H})$
      
          \State $\mathbf{S} \gets \mathcal{S}_{\frac{\lambda}{r_1}}(\mathbf{X} - \mathbf{L} + \frac{1}{r_1}\mathbf{Z}_1)$
          \State $\mathbf{W} \gets r_2 (\gamma \bm{\phi} + r_2 \mathbf{I})^{-1}(L - \frac{1}{r_2}Z_2)$
          
          \State $\mathbf{Z}_1 \gets \mathbf{Z}_1 + r_1(\mathbf{X} - \mathbf{L} - \mathbf{S})$
          \State $\mathbf{Z}_2 \gets \mathbf{Z}_2 + r_2(\mathbf{W} - \mathbf{L})$
      
          \If{$||\mathbf{X} - \mathbf{L} - \mathbf{S}||_{F} \leqslant \varepsilon$}
              \State break\
          \EndIf
      
      \EndFor
      
      \State \Return $\mathbf{L}$, $\mathbf{S}$
  \end{algorithmic}
  \end{algorithm}


\section{Experimental setup}

For the benchmark, we compared these algorithms on two different tasks: low-rank recovery for corrupted data and clustering. For the low-rank recovery task, we used the CMU PIE dataset which is a dataset of images of faces with different angles and lighting. For the clustering task, we used the AT\&T dataset which is another dataset of images of faces.

\subsection{Low-rank recovery for corrupted data}

For this task, we used the PIE dataset, and more specifically 20 64$\times$64 images of the same 40 people under different lighting with a constant camera angle, and corrupted them by adding a single random black block of 25\% of the image dimensions to each sample. We then used the different algorithms to recover the concatenation of the original images from the concatenation of the corrupted ones. The main hypothesis here is that this long concatenation matrix will be well approximated by a low-rank matrix, since the difference in lighting and corruption is supposed to be sparse. We evaluated the different methods mostly by visual inspection of the results, but also by computing the mean squared error between the original and recovered images.

\subsection{Clustering}

For this task, we used the AT\&T dataset, and more specifically 400 112$\times$92 images aggregated in 40 equally sampled classes (each class corresponding to one individual). We then compute the low-rank approximation of the concatenation of the images, and used the resulting matrix as a feature matrix for clustering. We then evaluated the different methods by computing the labels produced by the KMeans algorithm trained on this feature matrix, and comparing them with the original labels of the dataset. We compared the different methods by computing the $ $ error between, as well as the Adjusted Rand Index (ARI) as well as the Normalized Mutual Information (NMI). These metrics can be easily computed given the confusion matrix and the raw labels:

\[ ARI = 2 \frac{TP \times TN - FN \times FP}{(TP + FN)\times(FN + TN) + (TP + FP)\times(FP + FN)} \]

\[ NMI = \frac{I(y_{\text{true}}, y_{\text{pred}})}{\sqrt{H(y_{\text{true}}) \times H(y_{\text{pred}})}} \]

where $TP$ is the number of true positives, $TN$ is the number of true negatives, $FP$ is the number of false positives, $FN$ is the number of false negatives, $I(y_{\text{true}}, y_{\text{pred}})$ is the mutual information between $y_{\text{true}}$ and $y_{\text{pred}}$, and $H(y)$ is the entropy of $y$.

\section{Results}




\section{Limitations}




\section{Conclusion}




%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\end{document}
\endinput